### 关联分析

**定义：**

关联分析，就是从大规模数据中，发现对象之间隐含关系与规律的过程，也称为关联规则学习。它用于寻找数据集中各项之间的关联关系。根据所挖掘的关联关系，可以从一个属性的信息来推断另一个属性的信息。当置信度达到某一阈值时，可以认为规则成立。

**关联分析包含2个过程：**

从数据集中寻找频繁项集，从频繁项集中生成关联规则。

1.寻找频繁项集

首先，我们需要找到所有的频繁项集，即经常出现在一起的对象集合。

2.从频繁项集中生成关联规则

当生成频繁项集后，生成关联规则会相对简单。我们只需要将每个频繁项集拆分成两个非空子集，就可以构成关联规则。当然，一个频繁项集拆分成两个非空子集可能有很多种方式，我们要考虑每一种不同的可能。

然后，我们针对每一个关联规则，分别计算其置信度，仅保留符合最小置信度的关联规则。



### 聚类分析

**定义：**

聚类分析是一种无监督学习，用于对未知类别的样本进行划分将它们按照一定的规则划分成若干个类簇，把相似(相关的)的样本聚在同一个类簇中，把不相似的样本分为不同类簇，从而分析样本之间内在的性质以及相互之间的联系规律。

**均值聚类分析算法：**

（1）初始化。随机选择k的样本作为初始聚类的中心。

（2）对样本进行聚类。针对初始化时选择的聚类中心，计算所有样本到每个中心的距离，默认欧式距离，将每个样本聚集到与其最近的中心的类中，构成聚类结果。

（3）计算聚类后的类中心，计算每个类的质心，即每个类中样本的均值，作为新的类中心。

（4）然后重新执行步骤（2）（3），直到聚类结果不再发生改变。

K均值聚类算法的时间复杂度是O(nmk)，n表示样本个数，m表示样本维数，k表示类别个数。



### 异常检测

异常检测的目标是发现与大部分对象不同的对象。异常检测是指在给定的一组无标签数据集{x(1)， x(2)， \...， x(m)}，针对这组数据集训练一个模型p(x)，来判定某个数据和数据集中大多数数据之间的相似程度(某个数据落在给定数据集中心区域的概率)，若某个数x(test)和大多数给定数据之间很相似，则p(x(test)) ≥e，说明数据x(test)无明显异常情况；否则p (x(test)) \< e，说明数据x(test)和大多数数据之间差异很大，说明给定的数据可能是一个异常数据。

**异常检测算法：**

（1）选择数据集中数据的一系列属性集合。

（2）根据属性集合建立数据集中数据的统计量如：均值，方差等。

（3）根据数据集中数据的统计量建立模型p(x)

1．**均值(mean)**，也称平均数，指在一组数据中所有数据之和再除以这组数据的个数。它是反映数据集中趋势的一项指标。均值是表示一组数据集中趋势的量数。

平均数=总数量÷总份数。

2．**中位数（Median）**又称中值，是按顺序排列的一组数据中居于中间位置的数。

将数据按大小顺序排列，位于数据中间的数即为中位数。

3、**众数（Mode）**指一组数据中出现最多的数。它表示在统计分布上具有明显集中趋势点的数值，代表数据的一般水平。

众数不用计算，在一组数据中出现次数最多的数值为众数。



均值非常明显的优点之一是，它能够利用所有数据的特征，而且比较好算。另外，在数学上，均值是使误差平方和达到最小的统计量，也就是说利用平均数代表数据，可以使二次损失最小。因为均值是由每一个数据计算得出的，所以它随着每一个数据的变化而变化，极易受到极端值的影响。

**均值优点：** 需要全组所有数据计算，计算代价简单。**缺点：**容易受到极端数值的影响。



中位数是通过排序得到的，它不受极端值的影响。即使部分数据的变化很大也对中位数没有影响，所以中位数是非常稳定的统计量。

**中位数优点：** 不易受数据中极端数值的影响。**缺点：**需要排序后得出计算代价比较大，没有完全利用数据所反映的信息。



众数反映了一组数据的最普遍的倾向。

**众数优点：**不易受数据中极端数值的影响。**缺点：**对众数做单调增（减）变化会导致众数发生变化，没有完全利用数据所反映的信息。



### 过拟合

**定义（现象）：**（简写，上节课手写，待补充）

模型在训练集表现非常好，却在测试集上表现不好，则这便是过拟合现象。

**成因：**（简写，上节课手写，待补充）

模型学习了没有用的数据集中的噪音。

**解决过拟合的方法：**（只写了3种，不止这3种）

（1）数据增强，指从数据源头获得更多的数据从而增加数据集中的数据量，或者通过旋转、平移等手段从一组数据得到多组数据。然而实际情况中，获得更多的数据往往比较困难。

（2）正则化：正则化指的是通过限制模型中的参数来达到指定的目的。在机器学习中，正则化是在损失函数后面加上正则罚项，使得通过最小化损失函数求解模型参数转变为通过最小化结构损失求解模型参数，进而选择损失函数小并且简单的模型。

（3）减小噪声：通过提高数据的质量，可以结合先验知识加工特征以及对数据中噪声进行剔除，从而减小过拟合。主要的方法有：数据清洗、数据剪枝等等。



**奥卡姆剃刀原则**

奥卡姆剃刀原则称为"如无必要，勿增实体"，即简单有效原理。在机器学习中，我们说在相同泛化误差下，优先选用较简单的模型。利用奥卡姆剃刀原则，使用简单的模型或者降低模型的复杂度也可以降低过拟合。



梯度下降法应用一阶泰勒展开，牛顿法应用二阶泰勒展开。







方差：？

期望：？



贝叶斯公式：$P(w|X) = P(X|w)P(w) / P(X)$

贝叶斯思想与频率学派的不同：

1. 贝叶斯在获得数据之前，对模型的参数有一个主观的先验分布；频率学派完全以数据为中心，没有先验。
2. 贝叶斯学派认为模型的参数是一个概率分布，不是一个固定的值；频率学派认为模型的参数是一个固定的值。
3. 但是在具体的实际应用计算中，把参数看成概率分布很难计算，为了计算方便，往往用一个值（均值，中位数，众数……）代替概率分布。